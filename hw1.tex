\documentclass[12pt]{article}
\usepackage{url,hyperref,amsmath,setspace,amssymb,amsthm,amsfonts}


\setlength{\oddsidemargin}{.25in}
\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6.25in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}

\newcommand{\heading}[5]{
   \renewcommand{\thepage}{#1-\arabic{page}}
   \noindent
   \begin{center}
   \framebox[\textwidth]{
     \begin{minipage}{0.9\textwidth} \onehalfspacing
       {\bf CS 290G -- Introduction to Modern Cryptography} \hfill #2

       {\centering \Large #5
       
       }\medskip

       {\it #3 \hfill #4}
     \end{minipage}
   }
   \end{center}
}

\newcommand{\handout}[3]{\heading{#1}{#2}{Instructor:
Stefano Tessaro}{Student: Shiyu Ji}{#3}}

\setlength{\parindent}{0in}

\newcommand{\eqdef}{\stackrel{def}{=}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\bits}{\{0,1\}}
\newcommand{\inr}{\in_{\mbox{\tiny R}}}
%\newcommand{\getsr}{\gets_{\mbox{\tiny R}}}
\newcommand{\getsr}{\stackrel{\$}{\gets}}
\newcommand{\st}{\mbox{ s.t. }}
\newcommand{\etal}{{\it et al }}
\newcommand{\into}{\rightarrow}

\newcommand{\Ex}{\mathbb{E}}
\newcommand{\e}{\epsilon}
\newcommand{\ee}{\varepsilon}
\newcommand{\ceil}[1]{{\lceil{#1}\rceil}}
\newcommand{\floor}[1]{{\lfloor{#1}\rfloor}}
\newcommand{\angles}[1]{\langle #1 \rangle}
\newcommand{\Com}{{\sf Com}}
\newcommand{\desc}{{\sf desc}}

\newcommand{\rightstep}[1]{%
$\underrightarrow{\quad #1 \quad}$ }

\newcommand{\leftstep}[1]{%
$\underleftarrow{\quad #1 \quad}$ }
\newcommand{\Adv}{\textsf{Adv}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theorems & Definitions


\newtheorem{theorem}{Theorem}[section]

\newtheorem{claim}[theorem]{Claim}
\newtheorem{subclaim}{Claim}[theorem]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{observation}[theorem]{Observation}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{example}[theorem]{Example}
\newtheorem{counterexample}[theorem]{Counterexample}
\newtheorem{algorithm1}[theorem]{Algorithm}
\newtheorem{protocol}[theorem]{Protocol}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{fact}[theorem]{Fact}

%\bibliographystyle{plain}

\begin{document}
\handout{1}{Due: Jan 29, 2016}{Homework 1}
\section{Task 1 - Negligible Functions}
Recall that a function $f : \N \mapsto \R_{\geq 0}$ is \emph{negligible} if for all $c\in\N$, there exists $n_c$ such that $f(n)\leq n^{-c}$ for all $n\geq n_c$.

{\bf a) [Points: 6]} Which ones of the following functions are negligible, and which ones are not? Prove your claim!
$$f_1(n) = 2^{-n}, \quad f_3(n) = 2^{-\log n}, \quad f_5(n) = n^{-\log n},$$
$$f_2(n) = n^{-100}, \quad f_4(n) = 2^{-\log^2 n}, \quad f_6(n) = (-1)^n.$$

{\bf Solution}: 
\begin{itemize}
\item $f_1$ is negligible. For all $c\in\N$, let $n_c = 2^{2c}$. Note that $2c^2-4^c<0$ for any $c\in\N$. Hence for any $n\geq n_c =2^{2c}$,
$$n^c\cdot 2^{-n} \leq n_c^c\cdot 2^{-n_c} = 2^{2c^2}\cdot 2^{-2^{2c}} = 2^{2c^2-4^c} < 1.$$
The first inequality holds since the function $h(n)=n^c\cdot 2^{-n}$ decreases over $n\geq 2^c$. It is easy to see $\log h(n) = c\log n -n$ decreases over $n\geq 2^c$, since $(\log h(n))' = c/n-1<0$ if $n\geq 2^c$. Thus we have for any $n\geq n_c =2^{2c}$, $2^{-n} < n^{-c}$.

\item $f_2$ is \emph{not} negligible. Let $c=1000$. There are infinite $n$'s s.t. $n^{-100} > n^{-1000}$. In fact, this holds for every $n>1$.

\item $f_3$ is \emph{not} negligible. Note that $f_3(n) = 2^{-\log n} = 1/n$. Let $c=2$. For any $n>1$, we have $1/n > n^{-c} = 1/(n^2)$.

\item $f_4$ is negligible. Note that $f_4(n) = 2^{-\log^2 n} = n^{-\log n}$. For all $c\in\N$, let $n_c = 2^c$. Then for any $n\geq n_c = 2^c$,
$$f_4(n) = n^{-\log n} \leq n^{-\log n_c} = n^{-\log 2^c} = n^{-c}.$$

\item $f_5$ is negligible, since $f_5(n) = f_4(n)$, which has been shown to be negligible.

\item $f_6$ is \emph{not} negligible. Let $c=1$. Note that when $n$ takes any even number larger than 1, we have $1/n < 1 = (-1)^n$. Hence there are infinite $n$'s s.t. $(-1)^n > n^{-c} = 1/n$.
\end{itemize}

{\bf b) [Points: 6]} Show that for any two negligible functions $f, g$, and any polynomial $p$ which is non-negative (i.e., $p(n)\geq 0$ on all $n\in\N$) the following functions are negligible:
$$n \mapsto f(n)+g(n), \quad n \mapsto p(f(n)) \textrm{ where $p$ has no constant term},$$
$$n \mapsto f(n)\cdot g(n), \quad n \mapsto p(n)\cdot f(n) \textrm{ for a polynomial $p(n)$}.$$

{\bf Solution}:
\begin{itemize}
\item $f(n)+g(n)$ is negligible.
\begin{proof}
Since $f$ and $g$ are negligible, for any $c+1\in \N$, there exists $n_{c+1,f}$ s.t. for any $n\geq n_{c+1,f}$, $f(n) \leq n^{-(c+1)}$, and there exists $n_{c+1,g}$ s.t. for any $n\geq n_{c+1,g}$, $g(n) \leq n^{-(c+1)}$. Thus, for any $n\geq \max\{n_{c+1,f}, n_{c+1,g}\}$, we have $f(n) \leq n^{-(c+1)}$ and $g(n) \leq n^{-(c+1)}$. Now given any $c\in\N$, let $n_c = \max\{3, n_{c+1,f}, n_{c+1,g}\}$. Then for any $n\geq n_c$, we have $f(n)+g(n) \leq 2n^{-(c+1)} = \frac{2}{n}n^{-c} < n^{-c}$, which satisfies the definition of a negligible function.
\end{proof}

\item For a \emph{non-negative} polynomial $p(n)$ with no constant term, $p(f(n))$ is negligible.
\begin{proof}
Suppose the degree of $p(n)$ is $d$, and $p(n) = \sum_{k=1}^d a_kn^k$, where each $a_k\in\R$. 
%Then $a_d\geq 0$; otherwise, $p(n)$ cannot be non-negative. 
$$\forall x\in [0,1], p(x) = \sum_{k=1}^d a_kx^k \leq \sum_{k=1}^d |a_k|x^k \leq \sum_{k=1}^d |a_k|x$$
Define the coefficient magnitude sum of $p$: $S_p = \sum_{k=1}^d |a_k|$. Then for all $f(n)\in[0,1]$, $p(f(n)) \leq S_p \cdot f(n)$.
On the other hand, since $f(n)$ is negligible, for any $c\in\N$, there exists $n_{c,f}$ s.t. for any $n\geq n_{c,f}$, we have $f(n)\leq n^{-c}$. 
For all $c\in\N$, let $n_c = \max\{\lceil S_p \rceil+1, n_{c+1,f}, 2, n_{1,f}\}$. Then for any $n\geq n_c$, we have $f(n)\leq 1/n<1$ and thus
$$p(f(n)) \leq S_p \cdot f(n) \leq S_p \cdot n^{-(c+1)} = \frac{S_p}{n}n^{-c} < n^{-c},$$
which satisfies the definition of a negligible function.
\end{proof}

\item $f(n)\cdot g(n)$ is negligible.
\begin{proof}
Since $f$ and $g$ are negligible, for any $c\in \N$, there exists $n_{c,f}$ s.t. for any $n\geq n_{c,f}$, $f(n) \leq n^{-c}$, and there exists $n_{c,g}$ s.t. for any $n\geq n_{c,g}$, $g(n) \leq n^{-c}$. Let $n_c = \max\{n_{c,f}, n_{c,g}\}$. Then for any $n\geq n_c$, we have $f(n) \leq n^{-c}$ and $g(n) \leq n^{-c}$. Hence $f(n)\cdot g(n) \leq n^{-2c} \leq n^{-c}$, which satisfies the definition of a negligible function.
\end{proof}

\item For a \emph{non-negative} polynomial $p(n)$, $p(n)\cdot f(n)$ is negligible.
\begin{proof}
Suppose the degree of $p(n)$ is $d$, and $p(n) = \sum_{k=0}^d a_kn^k$, where each $a_k\in\R$.
Since $p(n)$ is non-negative, we have $a_d \geq 0$. Otherwise, a sufficiently large $n$ can give negative $p(n)$, which is dominated by the term $a_dn^d$.
Define the maximum coefficient: $a \eqdef \max_{k=0}^d \{a_k\} \geq a_d \geq 0$. Note that 
$$p(n) = \sum_{k=0}^d a_kn^k \leq a \sum_{k=0}^d n^k = a \frac{n^{d+1}-1}{n-1} \leq a \frac{n^{d+1}}{n-1}.$$
When $n-1>a$, we have $p(n) < n^{d+1}$. 

Since $f(n)$ is negligible, for any $c\in\N$, there exists $n_{c,f}$ s.t. for any $n\geq n_{c,f}$, $f(n) \leq n^{-c}$. 
For any $c\in\N$, let $n_c = \max\{a+2, n_{c+d+1,f}\}$. Then for any $n\geq n_c$, we have
$$p(n)\cdot f(n) < n^{d+1} f(n) \leq n^{d+1} n^{-(c+d+1)} = n^{-c},$$
which satisfies the definition of a negligible function.
\end{proof}
\end{itemize}

\section{Task 2 - One-Way Functions}
Let $f, g : \bits^\lambda \mapsto \bits^\lambda$ be both one-way functions. Which of the following statements
are true and which ones are false? Formally justify your claims â€“ i.e., either with a proof
of one-wayness or a counterexample.

{\bf a) [Points: 2]} $x \mapsto f(x) \oplus g(x)$, where $\oplus$ denotes bitwise xor.

{\bf Claim}: $x \mapsto f(x) \oplus g(x)$ is \emph{not} one-way.

{\bf Counterexample}:
Consider $f(x) = g(x)$, i.e., $f$ and $g$ are the same one-way function. Then we have
$$\forall x\in\bits^\lambda, f(x) \oplus g(x) = f(x) \oplus f(x) = 0^\lambda.$$
Given any $h(x) \eqdef f(x) \oplus g(x)$, the adversary just chooses $x' \getsr \bits^\lambda$ and inverts $h(x)$ (i.e. $h(x')=h(x)$) with advantage 1.

{\bf b) [Points: 2]} $(x, y) \mapsto f(x) || g(y)$.

{\bf Claim}: $(x, y) \mapsto f(x) || g(y)$ is one-way.
\begin{proof}
Assume by contradiction $h(x, y) \eqdef f(x) || g(y)$ is not one-way. Then there exists a PPT adversary $A$ s.t. the advantage $\epsilon(\lambda) \eqdef \Adv_{h,\lambda}^{owf}(A)$ is non-negligible. We are going to build an algorithm $A'$ and argue that $A'$ can invert $f(x)$ with non-negligible advantage $\Adv_{f,\lambda}^{owf}(A')$. Given input $z \in\bits^{\lambda}$, the construction of $A'$ is given as following:
\begin{quote}
Adversary $A' (z)$:
\begin{enumerate}
\item $y \getsr \bits^\lambda$.
\item $(x', y') \getsr A(z || g(y))$.
\item {\bf output} $x'$.
\end{enumerate}
\end{quote}
We can estimate the advantage of $A'$:
$$
\begin{aligned}
& \Adv_{f,\lambda}^{owf}(A') \\
=& \Pr[x\getsr \bits^\lambda, x' \getsr A'(f(x)) : f(x')=f(x)] \\
=& \Pr[x\getsr \bits^\lambda, y\getsr \bits^\lambda, (x', y') \getsr A(f(x) || g(y)) : f(x')=f(x)] \\
\geq & \Pr[x\getsr \bits^\lambda, y\getsr \bits^\lambda, (x', y') \getsr A(f(x) || g(y)) : f(x')=f(x), g(y')=g(y)] \\
=& \Pr[x\getsr \bits^\lambda, y\getsr \bits^\lambda, (x', y') \getsr A(h(x, y)) : h(x', y')=h(x, y)] \\
=& \Adv_{h,\lambda}^{owf}(A) = \epsilon(\lambda).
\end{aligned}
$$
Hence $A'$ has a non-negligible advantage.
\end{proof}

{\bf c) [Points: 2]} $x \mapsto f(g(x))$.

{\bf Claim}: $x \mapsto f(g(x))$ is \emph{not} one-way.

{\bf Counterexample}: Suppose $h, j : \bits^\lambda \mapsto \bits^\lambda$ are one-way functions (not permutations) and w.l.o.g $0^\lambda$ is not in the range of $j$. We construct two functions $f,g : \bits^{2\lambda} \mapsto \bits^{2\lambda}$ as following, given any inputs $x, y \in \bits^\lambda$:
$$g(x,y) \eqdef h(x) || h(x).$$
$$f(x,y) \eqdef
\begin{cases}
0^{2\lambda} & \textrm{if $x=y$,} \\
(j(x),j(y)) & \textrm{otherwise.}
\end{cases}$$
Apparently $f(g(z)) = 0^{2\lambda}$ for all $z\in\bits^{2\lambda}$. Thus $h\eqdef f\circ g$ is \emph{not} one-way over $\bits^{2\lambda}$.
We claim that both $f$ and $g$ are one-way.
\begin{lemma}
Given one way function $h : \bits^\lambda \mapsto \bits^\lambda$, $g(x,y) \eqdef h(x) || h(x)$ is also one-way.
\end{lemma}
\begin{proof}
Assume by contradiction $g$ is not one-way. Then there exists a PPT adversary $A$ s.t. the advantage $\epsilon(\lambda) \eqdef \Adv_{g,\lambda}^{owf}(A)$ is non-negligible. It suffices to build an algorithm $A'$ s.t. the advantage $\Adv_{h,\lambda}^{owf}(A')$ is also non-negligible. Given any input $y=h(x)$, we give the construction of $A'$ as following:
\begin{quote}
Adversary $A' (y)$:
\begin{enumerate}
\item $(x', y') \getsr A(y || y)$.
\item {\bf output} $x'$.
\end{enumerate}
\end{quote}
We can estimate the advantage of $A'$:
$$
\begin{aligned}
&\Adv_{h,\lambda}^{owf}(A') = \Pr[x\getsr \bits^\lambda, x' \getsr A'(h(x)) : h(x')=h(x)] \\
=& \Pr[(x,y)\getsr \bits^{2\lambda}, x' \getsr A'(h(x)) : h(x')=h(x)] \\
=& \Pr[(x,y)\getsr \bits^{2\lambda}, (x', y') \getsr A(h(x) || h(x)) : h(x')=h(x)] \\
=& \Pr[(x,y)\getsr \bits^{2\lambda}, (x', y') \getsr A(h(x) || h(x)) : h(x')||h(x') = h(x)||h(x)] \\
=& \Pr[(x,y)\getsr \bits^{2\lambda}, (x', y') \getsr A(g(x, y)) : g(x', y') = g(x, y)] \\
=& \Adv_{g,\lambda}^{owf}(A) = \epsilon(\lambda),
\end{aligned}
$$
which is also non-negligible.
\end{proof}

\begin{lemma}
Given one way function $j : \bits^\lambda \mapsto \bits^\lambda$ s.t. $0^\lambda \not\in \mathrm{range}(j)$, the function $f : \bits^{2\lambda} \mapsto \bits^{2\lambda}$ given by
$$f(x,y) \eqdef
\begin{cases}
0^{2\lambda} & \textrm{if $x=y$,} \\
(j(x),j(y)) & \textrm{otherwise.}
\end{cases}$$
is also one-way.
\end{lemma}
\begin{proof}
Assume by contradiction $f(x,y)$ is not one-way. Then there exists a PPT adversary $A$ s.t. the advantage $\epsilon(\lambda) = \Adv_{f,\lambda}^{owf}(A)$ is non-negligible. It suffices to build an algorithm $A'$ s.t. the advantage $\Adv_{j,\lambda}^{owf}(A')$ is also non-negligible. Given any input $y_1 = j(x_1)$, we give the construction of $A'$ as following:
\begin{quote}
Adversary $A' (y_1)$:
\begin{enumerate}
\item $x_2 \getsr \bits^\lambda$.
\item $(x_1',x_2') \getsr A(y_1,j(x_2))$.
\item {\bf output} $x_1'$.
\end{enumerate}
\end{quote} 
We can estimate the advantage of $A'$:
$$
\begin{aligned}
&\Adv_{j,\lambda}^{owf}(A') \\
=& \Pr[x_1 \getsr \bits^\lambda, x_1' \getsr A'(j(x_1)) : j(x_1') = j(x_1)] \\
=& \Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)) : j(x_1') = j(x_1)] \\
=& \Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)) : x_1=x_2, j(x_1') = j(x_1)] \\
&+\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)) : x_1\not=x_2, j(x_1') = j(x_1)].
\end{aligned}
$$
Note that the first term of probability 
$$\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)) : x_1=x_2, j(x_1') = j(x_1)]$$
$$\leq \Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)) : x_1=x_2] = 2^{-\lambda},$$
is negligible. From Task 1, it suffices to show that the second term of probability
$$\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)) : x_1\not=x_2, j(x_1') = j(x_1)]$$
is non-negligible.
By conditional probability,
$$
\begin{aligned}
&\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)) : x_1\not=x_2, j(x_1') = j(x_1)]\\
=&\Pr[(x_1, x_2) \getsr \bits^{2\lambda}: x_1\not=x_2]\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)), \\
&x_1\not=x_2 : j(x_1') = j(x_1)]\\
=&(1-2^{-\lambda})\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)), x_1\not=x_2 : j(x_1') = j(x_1)].
\end{aligned}
$$
Similarly by Task 1, we only need to show that the probability
\begin{equation}
\label{eq:1}
\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)), x_1\not=x_2 : j(x_1') = j(x_1)]
\end{equation}
is non-negligible. On the other hand, again by using conditional probabilities,
$$
\begin{aligned}
&\epsilon(\lambda) = \Adv_{f,\lambda}^{owf}(A) \\
=&\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(f(x_1,x_2)) : f(x_1',x_2') = f(x_1,x_2)]\\
=&(1-2^{-\lambda})\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(f(x_1,x_2)), x_1\not=x_2: f(x_1',x_2') = f(x_1,x_2)]\\
&+2^{-\lambda}\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(f(x_1,x_2)), x_1=x_2: f(x_1',x_2') = f(x_1,x_2)].
\end{aligned}
$$
Define 
$$\epsilon'(\lambda) \eqdef \Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(f(x_1,x_2)), x_1\not=x_2: f(x_1',x_2') = f(x_1,x_2)],$$
which is non-negligible. Since if $x_1\not=x_2$, we have $f(x_1,x_2)\not= 0^{2\lambda}$, and thus $f(x_1,x_2) = f(x_1',x_2') \not=0^{2\lambda}$ implies $j(x_1)=j(x_1')$. Hence we can show the probability in (\ref{eq:1}) is non-negligible.
$$
\begin{aligned}
&\Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(j(x_1),j(x_2)), x_1\not=x_2 : j(x_1') = j(x_1)] \\
=& \Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(f(x_1,x_2)), x_1\not=x_2 : j(x_1') = j(x_1)] \\
\geq & \Pr[(x_1, x_2) \getsr \bits^{2\lambda}, (x_1', x_2') \getsr A(f(x_1,x_2)), x_1\not=x_2 : f(x_1,x_2) = f(x_1',x_2')] \\
=& \epsilon'(\lambda).
\end{aligned}$$
\end{proof}

\section{Task 3 - Hard-Core Predicates}
Let $f : \bits^\lambda \mapsto \bits^{\lambda'}$ be a permutation family, and let $P : \bits^\lambda \mapsto \bits$ be a family of predicates. Then, recall that $P$ is a hardcore predicate for $f$ if for all PPT $A$, we have that
$$\Adv_{f,P,\lambda}^{pred}(A) = 2 \left| \Pr[x\getsr\bits^\lambda : A(f(x)) = P(x)] -\frac{1}{2}\right|$$
is negligible.

{\bf a) [Points: 8]} Prove, giving an explicit reduction, that if $f : \bits^\lambda \mapsto \bits^{\lambda'}$ is a \emph{permutation}, and there exists some hardcore predicate $P$ for $f$, then $f$ must be one way.
\begin{proof}
Assume by contradiction $f$ is not one-way. Then there exists a PPT adversary $A$ s.t. the advantage $\Adv_{f,\lambda}^{owf}(A)$ is non-negligible. It suffices to build a distinguisher $D$ s.t. the advantage $\Adv_{f,P,\lambda}^{pred}(D)$ is also non-negligible. Given any input $y=f(x)$, we give the construction of $D$ as following:
\begin{quote}
Distinguisher $D(y)$:
\begin{enumerate}
\item $x' \getsr A(y)$.
\item {\bf if} $f(x') = y$ {\bf then output} $P(x')$.
\item {\bf else} $r\getsr\bits$, and {\bf output} $r$. 
\end{enumerate}
\end{quote}
Now we can compute the advantage of $D$:
$$
\begin{aligned}
&\Adv_{f,P,\lambda}^{pred}(D) = 2 \left| \Pr[x\getsr\bits^\lambda : D(f(x)) = P(x)] -\frac{1}{2}\right| \\
=& 2 \bigg| \Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)) : f(x')=f(x), D(f(x)) = P(x)] \\
&+ \Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)) : f(x')\not=f(x), D(f(x)) = P(x)] -\frac{1}{2}\bigg|.
\end{aligned}
$$
We can compute both the terms of probability.
$$
\begin{aligned}
&\Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)) : f(x')=f(x), D(f(x)) = P(x)] \\
&+\Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)) : f(x')\not=f(x), D(f(x)) = P(x)] \\
=&\Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)) : f(x')=f(x)]\\
&\cdot\Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)), f(x')=f(x): P(x') = P(x)] \\
&+\Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)) : f(x')\not=f(x)]\\
&\cdot\Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)), f(x')\not=f(x), r\getsr\bits: r = P(x)] \\
\end{aligned}
$$
Note that $\Adv_{f,\lambda}^{owf}(A) \eqdef \Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)) : f(x')=f(x)]$, and since $f$ is one-to-one, $f(x')=f(x)$ iff $x'=x$. Hence we have
$$
\begin{aligned}
&\Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)) : f(x')=f(x), D(f(x)) = P(x)] \\
&+\Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)) : f(x')\not=f(x), D(f(x)) = P(x)] \\
=&\Adv_{f,\lambda}^{owf}(A)\cdot\Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)), x'=x: P(x') = P(x)] \\
&+(1-\Adv_{f,\lambda}^{owf}(A))\cdot\Pr[x\getsr\bits^\lambda, x'\getsr A(f(x)), f(x')\not=f(x), r\getsr\bits: r = P(x)] \\
=&\Adv_{f,\lambda}^{owf}(A)\cdot 1 + (1-\Adv_{f,\lambda}^{owf}(A))/2 = \frac{1}{2} + \frac{1}{2}\Adv_{f,\lambda}^{owf}(A).
\end{aligned}
$$
Thus
$$\Adv_{f,P,\lambda}^{pred}(D) = 2\left| \frac{1}{2} + \frac{1}{2}\Adv_{f,\lambda}^{owf}(A)-\frac{1}{2} \right| = \Adv_{f,\lambda}^{owf}(A),$$
which is non-negligible.
\end{proof}

{\bf b) [Points: 2]} Why is it necessary in a) that $f$ is a permutation? Is the statement true for
general $f$? Justify your answer!

{\bf Claim}: The statement is not true for general $f$.

{\bf Counterexample}: Suppose $f : \bits^\lambda \mapsto \bits$ is defined by $f(x) = 0$ for any $x\in\bits^\lambda$. Apparently $f$ is not one-way, but we claim the bitwise xor $\oplus_i x_i$ is a hardcore predicate of $f$. Note that the predicate $\oplus_i x_i$ is uniformly distributed over $\bits$ if $x\getsr\bits^\lambda$. Since given any input $f(x)$ which is always 0, any adversary has no knowledge about $x$, and thus the best way to guess $\oplus_i x_i$ is based on the statistical distribution of the predicate. However, it is a uniform distribution, and thus any adversary cannot have non-negligible advantage.


\section{Task 4 â€“ Pseudorandom Generators}
Throughout this task let $G : \bits^\lambda \mapsto \bits^{n(\lambda)}$ for $\lambda<n=n(\lambda)$ be a PRG.

{\bf a) [Points: 4]} Prove that $G' : \bits^{\lambda + n(\lambda)} \mapsto \bits^{2n(\lambda)}$ such that
$$G'(x||y) = (G(x)\oplus y)||y$$
for all $x\in\bits^\lambda$ and $y\in \bits^{n(\lambda)}$ is a pseudorandom generator.
\begin{proof}
Assume by contradiction $G'$ as constructed above is not a PRG. Then there exists a PPT distinguisher $A$ s.t. the advantage $\Adv_{G',\lambda}^{prg}(A)$ is non-negligible. It suffices to build a PPT distinguisher $A'$ s.t. the advantage $\Adv_{G,\lambda}^{prg}(A')$ is also non-negligible. Given any input $y = G(s)$, we give the construction of $A'$ as following:
\begin{quote}
Distinguisher $A'(y)$:
\begin{enumerate}
\item $z\getsr \bits^{n(\lambda)}$.
\item $b' \getsr A((y\oplus z)||z)$.
\item {\bf output} $b'$.
\end{enumerate}
\end{quote}
Now we can estimate the advantage of $A'$ as following:
$$
\begin{aligned}
&\Adv_{G,\lambda}^{prg}(A') = \left| \Pr[s\getsr\bits^\lambda : A'(G(s))=1] - \Pr[y\getsr\bits^{n(\lambda)} : A'(y)=1] \right| \\
=&\bigg| \Pr[s\getsr\bits^\lambda, z\getsr \bits^{n(\lambda)} : A((G(s)\oplus z)||z)=1]\\
 &- \Pr[y\getsr\bits^{n(\lambda)}, z\getsr \bits^{n(\lambda)} : A((y\oplus z)||z)=1] \bigg| \\
=&\bigg| \Pr[s\getsr\bits^\lambda, z\getsr \bits^{n(\lambda)} : A(G'(s||z))=1] - \Pr[y''\getsr\bits^{2n(\lambda)} : A(y'')=1] \bigg| \\
=&\Adv_{G',\lambda}^{prg}(A),
\end{aligned}
$$
which is non-negligible. Note that in above if $y$ and $z$ are sampled at uniformly random, then $y'' \eqdef (y\oplus z)||z$ is also uniformly distributed. This can be shown by direct probability computation: let $x_1$ and $x_2$ be \emph{any} string in $\bits^{n(\lambda)}$, and $y,z \getsr\bits^{n(\lambda)}$. Then we have
$$\Pr[(y\oplus z)||z = x_1||x_2] = \Pr[y\oplus z = x_1 | z=x_2]\Pr[z=x_2]$$
$$=\Pr[y = x_1\oplus x_2]\Pr[z=x_2] = 2^{-2n(\lambda)}.$$
\end{proof}

{\bf b) [Points: 6]} Prove that the function $G'' : \bits^{2\lambda} \mapsto \bits^{2n(\lambda)}$ such that
$$G''(x||y) = G(x)||G(y)$$
for all $x,y\in\bits^\lambda$ is a pseudorandom generator.
\begin{proof}
Assume by contradiction $G''$ is not a PRG. Then there exists a PPT distinguisher $D$ s.t. the advantage $\Adv_{G'',\lambda}^{prg}(D)$ is non-negligible. We first give the construction of the hybrids:
\begin{itemize}
\item $H_0$: $G(x)||G(y)$, where $x, y \getsr\bits^\lambda$.
\item $H_1$: $z||G(y)$, where $z\getsr\bits^{n(\lambda)}$, $y\getsr\bits^\lambda$.
\item $H_2$: $z||w$, where $z,w\getsr\bits^{n(\lambda)}$.
\end{itemize}
Note that $H_0$ is the output distribution of $G''$ given uniformly random seeds as inputs, and $H_2 = U_{2n(\lambda)}$.
Since $D$ can distinguish between $H_0$ and $H_2$, by Hybrid Lemma, $D$ can distinguish between $H_0$ and $H_1$, \emph{or} distinguish between $H_1$ and $H_2$. If $D$ can distinguish $H_0$ and $H_1$ with a non-negligible advantage $\epsilon_1(\lambda) \eqdef \Adv_{H_0,H_1}^{dist}(D)$, then by using $D$ we can construct a distinguisher $D'$ s.t. $\Adv_{G,\lambda}^{prg}(D')$ is non-negligible:
\begin{quote}
Distinguisher $D' (x)$, where $x=G(s)$, $s\in\bits^\lambda$
\begin{enumerate}
\item $y\getsr\bits^{\lambda}$.
\item $b' \getsr D(x||G(y))$.
\item {\bf output} $b'$.
\end{enumerate}
\end{quote}
The advantage of $D'$ is 
$$
\begin{aligned}
& \Adv_{G,\lambda}^{prg}(D') \\
=& \bigg| \Pr[s\getsr\bits^\lambda : D'(G(s))=1] - \Pr[z\getsr\bits^{n(\lambda)}: D'(z)=1] \bigg| \\
=& \bigg| \Pr[s\getsr\bits^\lambda, y\getsr\bits^\lambda : D(G(s)||G(y))=1]\\
& - \Pr[z\getsr\bits^{n(\lambda)}, y\getsr\bits^\lambda: D(z||G(y))=1] \bigg| \\
=& \epsilon_1(\lambda),
\end{aligned}
$$
which is non-negligible. For the other case, if $D$ can distinguish $H_1$ and $H_2$ with a non-negligible advantage $\epsilon_2(\lambda) \eqdef \Adv_{H_1,H_2}^{dist}(D)$, then by using $D$ we can construct another distinguisher $D''$ s.t. $\Adv_{G,\lambda}^{prg}(D'')$ is also non-negligible:
\begin{quote}
Distinguisher $D'' (x)$, where $x=G(s)$, $s\in\bits^\lambda$
\begin{enumerate}
\item $z\getsr\bits^{n(\lambda)}$.
\item $b' \getsr D(z||x)$.
\item {\bf output} $b'$.
\end{enumerate}
\end{quote}
The advantage of $D''$ is
$$
\begin{aligned}
& \Adv_{G,\lambda}^{prg}(D'') \\
=& \bigg| \Pr[s\getsr\bits^\lambda : D''(G(s))=1] - \Pr[x\getsr\bits^{n(\lambda)}: D''(x)=1] \bigg| \\
=& \bigg| \Pr[s\getsr\bits^\lambda, z\getsr\bits^{n(\lambda)} : D(z||G(s))=1]\\
& - \Pr[x\getsr\bits^{n(\lambda)}, z\getsr\bits^{n(\lambda)}: D(z||x)=1] \bigg| \\
=& \epsilon_2(\lambda),
\end{aligned}
$$
which is non-negligible.
Hence for both cases we can construct distinguishers against $G$ with non-negligible advantage, implying $G$ is not PRG. Contradiction.
\end{proof}

{\bf c) [Points: 6]} Prove that the function $G''' : \bits^{\lambda} \mapsto \bits^{2n(\lambda)}$ such that
$$G'''(x) = G(x)||G(\overline{x})$$
for all $x\in\bits^\lambda$ is \emph{not} a pseudorandom generator, where $\overline{x}$ is the bit-wise complement of $x$ (i.e., each bit is flipped).
\begin{proof}
Consider the Goldreich-Levin PRG construction $G^\pi : \bits^{2\lambda} \mapsto \bits^{2\lambda+1}$ given by
$$G^\pi(x||r) \eqdef \pi(x)||r||\angles{x,r}$$
where $\pi$ is a OWP over $\bits^\lambda$, $x,r \in \bits^\lambda$ and $\angles{x,r}$ denotes the mod 2 inner product of $x$ and $r$:
$$\angles{x,r} \eqdef \sum_{k=1}^\lambda x_ir_i\mod 2.$$
Now the function $G''' : \bits^{2\lambda} \mapsto \bits^{4\lambda+2}$ is
$$G'''(x||r) = G^\pi(x||r)||G^\pi(\overline{x||r}) = \pi(x)||r||\angles{x,r}||\pi(\overline{x})||\overline{r}||\angles{\overline{x},\overline{r}}.$$
Note that $r$ gives two complementary segments in the output of $G'''$. This gives us chance to build an efficient distinguisher as following:
\begin{quote}
Distinguisher $D(y_1||y_2||b_1||y_3||y_4||b_2)$: (here $y_1,y_2,y_3,y_4 \in \bits^\lambda$, $b_1,b_2\in\bits$)
\begin{enumerate}
\item {\bf if} $y_2 \oplus y_4 = 1^\lambda$ {\bf then output} 1.
\item {\bf else output} 0.
\end{enumerate}
\end{quote}
We compute the advantage of $D$:
$$
\begin{aligned}
&\Adv_{G''',\lambda}^{prg}(D) = \bigg| \Pr[s\getsr \bits^{2\lambda}: D(G'''(s))=1] - \Pr[x\getsr \bits^{4\lambda+2}: D(x)=1] \bigg| \\
=& \bigg| 1 - \Pr[y_2,y_4 \getsr\bits^\lambda: y_2 \oplus y_4 = 1^\lambda] \bigg| = 1 - 2^{-\lambda},
\end{aligned}
$$
which is a overwhelming advantage on $\lambda$. Hence the construction $D$ is an efficient distinguisher.
\end{proof}


\section{Task 5 â€“ Distinguishing Advantages}
Let $P_0, P_1 : \mathcal{X} \mapsto [0,1]$ be probability distributions on a finite set $\mathcal{X}$, i.e., we have $\sum_{x\in\mathcal{X}}P_b(x) = 1$ for $b\in\bits$. As in class, denote by $x\getsr P_b$ the process of sampling a value $x$ according to the probability distribution $P_b$, i.e., every value $x$ is taken with probability $P_b(x)$.

We consider the following two definitions of distinguishing advantage:
\begin{enumerate}
\item {\bf Comparing probabilities.} The first definition compares the probabilities of a distinguisher
outputting one in two different experiments. I.e., we define
$$\Adv_{P_0,P_1}^{dist}(D) = \bigg| \Pr[x\getsr P_0: D(x)=1] - \Pr[x\getsr P_1: D(x)=1] \bigg|.$$
\item {\bf Guessing Game.} The second definition considers a single random experiment where
the distinguisher is given a sample from either of the two distributions, and its task
is to guess which one of the distributions was used. Formally,
$$\Adv_{P_0, P_1}^{dist'}(D) = 2\bigg| \Pr[b \getsr \bits, x\getsr P_b: D(x)=b] -\frac{1}{2} \bigg|.$$
\end{enumerate}
{\bf a) [Points: 4]} Prove that the following identity holds for all distinguishers $D$:
$$\Adv_{P_0,P_1}^{dist}(D) = \Adv_{P_1,P_0}^{dist'}(D).$$
\begin{proof}
We may directly prove by definitions.
$$\begin{aligned}
&\Adv_{P_1, P_0}^{dist'}(D) = 2\bigg| \Pr[b \getsr \bits, x\getsr P_b: D(x)=b] -\frac{1}{2} \bigg| \\
=& 2\bigg| \Pr[b \getsr \bits, x\getsr P_b: b=0, D(x)=b] + \Pr[b \getsr \bits, x\getsr P_b: b=1, D(x)=b] -\frac{1}{2} \bigg|.
\end{aligned}
$$
Using conditional probability, one has 
$$\begin{aligned}
& \Pr[b \getsr \bits, x\getsr P_b: b=0, D(x)=b] \\
=& \Pr[b \getsr \bits : b=0] \Pr[b \getsr \bits, x\getsr P_b: D(x)=b | b=0] \\
=& \frac{1}{2} \Pr[x\getsr P_0: D(x)=0].
\end{aligned}$$
Similarly, 
$$\Pr[b \getsr \bits, x\getsr P_b: b=1, D(x)=b] = \frac{1}{2} \Pr[x\getsr P_1: D(x)=1]$$
Hence
$$\begin{aligned}
&\Adv_{P_1, P_0}^{dist'}(D) \\
=& 2\bigg| \frac{1}{2} \Pr[x\getsr P_0: D(x)=0] + \frac{1}{2} \Pr[x\getsr P_1: D(x)=1] -\frac{1}{2} \bigg| \\
=& \bigg| \Pr[x\getsr P_0: D(x)=0] + \Pr[x\getsr P_1: D(x)=1] -1 \bigg| \\
=& \bigg| \Pr[x\getsr P_1: D(x)=1] - \Pr[x\getsr P_0: D(x)=1] \bigg| \\
=& \Adv_{P_0,P_1}^{dist}(D).
\end{aligned}
$$
Note that $\Pr[x\getsr P_0: D(x)=1] + \Pr[x\getsr P_0: D(x)=0] = 1$.
\end{proof}

{\bf b) [Points: 4]} The \emph{statistical distance} between $P_0$ and $P_1$ is the quantity
$$SD(P_0, P_1) = \frac{1}{2}\sum_{x\in\mathcal{X}}|P_0(x) - P_1(x)|.$$
Exhibit a distinguisher $D$ such that $\Adv_{P_0,P_1}^{dist}(D) = SD(P_0,P_1)$. If both distributions are
on $\mathcal{X} = \bits^\lambda$, is your distinguisher efficient (i.e., polynomial-time) in $\lambda$?
\begin{lemma}
$$SD(P_0, P_1) = \sum_{x\in\mathcal{X}, P_0(x)\geq P_1(x)} (P_0(x) - P_1(x)).$$
\end{lemma} 
\begin{proof}
By definition,
$$SD(P_0, P_1) = \frac{1}{2}\sum_{x\in\mathcal{X}, P_0(x)\geq P_1(x)} (P_0(x) - P_1(x)) + \frac{1}{2}\sum_{x\in\mathcal{X}, P_0(x) < P_1(x)} (P_1(x) - P_0(x)).$$
Note that 
$$\sum_{x\in\mathcal{X}, P_0(x) < P_1(x)} P_0(x) + \sum_{x\in\mathcal{X}, P_0(x) \geq P_1(x)} P_0(x) = 1.$$
$$\sum_{x\in\mathcal{X}, P_0(x) < P_1(x)} P_1(x) + \sum_{x\in\mathcal{X}, P_0(x) \geq P_1(x)} P_1(x) = 1.$$
Thus
$$SD(P_0, P_1) = \frac{1}{2}\sum_{x\in\mathcal{X}, P_0(x)\geq P_1(x)} (P_0(x) - P_1(x))$$
$$ + \frac{1}{2} \left((1-\sum_{x\in\mathcal{X}, P_0(x) \geq P_1(x)} P_1(x))-(1-\sum_{x\in\mathcal{X}, P_0(x) \geq P_1(x)} P_0(x))\right).$$
That is,
$$SD(P_0, P_1) = \sum_{x\in\mathcal{X}, P_0(x)\geq P_1(x)} (P_0(x) - P_1(x)).$$
\end{proof}
Now we give the construction of the distinguisher $D(x)$ given the input $x\getsr P_b$ for any bit $b$:
\begin{quote}
Distinguisher $D(x)$:
\begin{enumerate}
\item {\bf if} $P_1(x) \geq P_0(x)$ {\bf then output} 1.
\item {\bf else output} 0.
\end{enumerate}
\end{quote}
We can compute the advantage of $D$:
$$
\begin{aligned}
& \Adv_{P_0,P_1}^{dist}(D) = \bigg| \Pr[x\getsr P_1: D(x)=1] - \Pr[x\getsr P_0: D(x)=1] \bigg| \\
=& \bigg| \sum_{x\in\mathcal{X}, P_1(x) \geq P_0(x)} P_1(x) - \sum_{x\in\mathcal{X}, P_1(x) \geq P_0(x)} P_0(x) \bigg| \\
=& \sum_{x\in\mathcal{X}, P_1(x) \geq P_0(x)} \bigg| P_1(x) - P_0(x) \bigg| = \sum_{x\in\mathcal{X}, P_1(x) \geq P_0(x)} (P_1(x) - P_0(x)).
\end{aligned}
$$
which is exactly $SD(P_1, P_0)$ by the lemma above. by the definition of statistical distance, it is easy to see $SD(P_1, P_0) = SD(P_0, P_1)$.

Our distinguisher itself is PPT. However, if the statistical distance between $P_0$ and $P_1$ is negligible on $\lambda$, then repeating $D$ by feeding independent inputs $x$ by only polynomial times cannot give a non-negligible advantage. Thus if $SD(P_0,P_1)$ is negligible on $\lambda$, our construction of $D$ may not be efficient (i.e., giving non-negligible $\Adv_{P_0,P_1}^{dist}(D)$ in polynomial time).

{\bf c) [Points: 8]} Consider a function $f : \bits^\lambda \mapsto \bits^{\lambda'}$ and a predicate $P : \bits^\lambda \mapsto \bits$.
Also let:
\begin{itemize}
\item $P_0$ be the distribution over pairs $(y, u)$ obtained by sampling $x\getsr\bits^\lambda$, $y\gets f(x)$, $u\gets P(x)$.
\item $P_1$ be the distribution over pairs $(y, u)$ obtained by sampling $x\getsr\bits^\lambda$, $y\gets f(x)$, $u\getsr \bits$.
\end{itemize}
Prove that for all distinguisher $D$ there exists an adversary $A$ such that
$$\Adv_{P_0,P_1}^{dist}(D) = \frac{1}{2}\Adv_{f,P,\lambda}^{pred}(A).$$
\begin{proof}
In fact, we can construct such an adversary $A$ given any distinguisher $D$ as following:
\begin{quote}
Adversary $A(y)$: (here $y=f(x)\in\bits^{\lambda'}$)
\begin{enumerate}
\item $u \getsr \bits$.
\item $b' \getsr D(y,u)$.
\item {\bf if} $b'=1$ {\bf then output} $u$.
\item {\bf else output} $1-u$.
\end{enumerate}
\end{quote}
Before the formal treatment of $\Adv_{f,P,\lambda}^{pred}(A)$. We first claim that for all hardcore predicate $P(x)$, its output is a uniformly random bit if the input $x$ is chosen at uniformly random.
\begin{lemma}
For any hardcore predicate $P : \bits^\lambda \mapsto \bits$ of OWF $f$,
$$\Pr[x\getsr\bits^\lambda, u\getsr\bits : u = P(x)] = \frac{1}{2}.$$
\end{lemma}
\begin{proof}
Assume by contradiction $x\getsr\bits^\lambda$, $P(x)$ is not uniformly distributed over $\bits$. Without loss of generality we assume $\Pr[x\getsr\bits^\lambda: P(x)=1] = \frac{1}{2}+\epsilon$, where $\epsilon>0$. Then we can construct an adversary $A$ who always guesses 1 when predicting the hardcore bit. Then the advantage
$$\Adv_{f,P,\lambda}^{pred}(A) = 2\bigg| \Pr[x\getsr\bits^\lambda: P(x)=1] -\frac{1}{2} \bigg| = 2\epsilon.$$
Since $\epsilon$ can be any positive \emph{constant}, it cannot be negligible. Hence $A$ is an efficient adversary against $P(x)$. Contradiction.
\end{proof}
Now we can compute the advantage of $A$:
$$
\begin{aligned}
&\Adv_{f,P,\lambda}^{pred}(A) \\
=& 2\bigg| \Pr[x\getsr\bits^\lambda: A(f(x)) = P(x)] - \frac{1}{2} \bigg| \\
=& 2\bigg| \Pr[x\getsr\bits^\lambda, u\getsr\bits, b'\getsr D(f(x),u): b'=1, u = P(x)]\\
&+ \Pr[x\getsr\bits^\lambda, u\getsr\bits, b'\getsr D(f(x),u): b'=0, 1-u = P(x)] - \frac{1}{2} \bigg|.
\end{aligned}
$$
Note that by the lemma above
$$\begin{aligned}
&\Pr[x\getsr\bits^\lambda, u\getsr\bits, b'\getsr D(f(x),u): b'=1, u = P(x)] \\
=& \Pr[x\getsr\bits^\lambda, u\getsr\bits : u = P(x)]\Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),P(x)): b'=1]\\
=&\frac{1}{2}\Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),P(x)): b'=1].
\end{aligned}$$
And similarly
$$\begin{aligned}
&\Pr[x\getsr\bits^\lambda, u\getsr\bits, b'\getsr D(f(x),u): b'=0, 1-u = P(x)] \\
=& \Pr[x\getsr\bits^\lambda, u\getsr\bits: 1-u=P(x)] \Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),1-P(x)): b'=0]\\
=&\frac{1}{2}\Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),1-P(x)): b'=0].
\end{aligned}$$
Thus
$$\begin{aligned}
&\Adv_{f,P,\lambda}^{pred}(A) \\
=& 2\bigg| \frac{1}{2}\Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),P(x)): b'=1] \\
&+ \frac{1}{2}\Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),1-P(x)): b'=0] - \frac{1}{2} \bigg|\\
=& \bigg| \Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),P(x)): b'=1] \\
&- \Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),1-P(x)): b'=1] \bigg|.
\end{aligned}$$
For the second probability, note that by conditional probabilities,
$$\begin{aligned}
& \Pr[x\getsr\bits^\lambda, u\getsr\bits, b'\getsr D(f(x),u): b'=1] \\
=&\Pr[x\getsr\bits^\lambda, u\getsr\bits, b'\getsr D(f(x),u): u=P(x), b'=1] \\
&+\Pr[x\getsr\bits^\lambda, u\getsr\bits, b'\getsr D(f(x),u): u=1-P(x), b'=1] \\
=&\frac{1}{2}\Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),P(x)): b'=1] \\
&+\frac{1}{2}\Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),1-P(x)): b'=1]. \\
\end{aligned}$$
Thus
$$\begin{aligned}
&\Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),1-P(x)): b'=1]\\
=&2\Pr[x\getsr\bits^\lambda, u\getsr\bits, b'\getsr D(f(x),u): b'=1]\\
&-\Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),P(x)): b'=1].
\end{aligned}$$
Now back to the advantage of $A$:
$$\begin{aligned}
&\Adv_{f,P,\lambda}^{pred}(A) \\
=& \bigg| \Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),P(x)): b'=1] \\
&- \Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),1-P(x)): b'=1] \bigg| \\
=& \bigg| \Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),P(x)): b'=1] \\
&- 2\Pr[x\getsr\bits^\lambda, u\getsr\bits, b'\getsr D(f(x),u): b'=1]\\
&+\Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),P(x)): b'=1].
\end{aligned}$$
Hence
$$\begin{aligned}
&\Adv_{f,P,\lambda}^{pred}(A) \\
=& 2\bigg| \Pr[x\getsr\bits^\lambda, b'\getsr D(f(x),P(x)): b'=1] \\
&-\Pr[x\getsr\bits^\lambda, u\getsr\bits, b'\getsr D(f(x),u): b'=1] \bigg| \\
=& 2\Adv_{P_0,P_1}^{dist}(D).
\end{aligned}$$

\end{proof}

\end{document}
